# -*- coding: utf-8 -*-
"""Обнаружение фальшивых новостей

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RpFvQuAuE-aNOPl0Um6ZTUcAiQW74JIp
"""

!pip install wordcloud
!pip install nltk spacy
!python -m spacy download en_core_web_sm

#Импортируем все необходимые библиотеки
import numpy as np
import pandas as pd
import itertools
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import re
import nltk
import spacy
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay

#Загрузим датафрейм из файла
df = pd.read_csv('/content/fake_news.csv')

#Получим кол-во новостей каждого типа и отобразим сравнение на графике
i = df.label.value_counts()

fig = go.Figure(data=[go.Bar(
            x=['Real','Fake'], y=i,
            text=i,
            textposition='auto',
        )])

fig.show()
#Как можно увидеть, датасет содержит примерно одиноковое кол-во и реальных и фейковых новостей

#Разделим исходный датафрейм на 2 для упрощения визуализации ключевых слов
real_df = df[df['label'] == 'REAL']
fake_df = df[df['label'] == 'FAKE']

#Подготавливаем текст, очищая его от лишних символов и приводя слова к единому значению

nlp = spacy.load('en_core_web_sm')  # Для лемматизации

# Функция для очистки текста
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)
    return text

# Функция для лемматизации текста
def lemmatize_text(text):
    doc = nlp(text)
    return " ".join([token.lemma_ for token in doc])

real_df['text'].apply(clean_text)
real_df['text'].apply(lemmatize_text)

fake_df['text'].apply(clean_text)
fake_df['text'].apply(lemmatize_text)

#Отобразим ключевые слова из 2 датафреймов

text_data_real = " ".join(text for text in real_df['text'])
text_data_fake = " ".join(text for text in fake_df['text'])

# Настройка облака слов
wordcloud_positive = WordCloud(width=800, height=400, background_color='white', colormap='Blues').generate(text_data_real)
wordcloud_negative = WordCloud(width=800, height=400, background_color='black', colormap='Reds').generate(text_data_fake)

# Выведем картинки сгенерированные вордклаудом
fig, ax = plt.subplots(1, 2, figsize = (20, 12))

ax[0].imshow(wordcloud_positive, interpolation='bilinear')
ax[1].imshow(wordcloud_negative, interpolation='bilinear')

ax[0].set_title('Real')
ax[1].set_title('Fake')

ax[0].axis("off")
ax[1].axis("off")

plt.show()

#Разбиваем данные на обучающие и тестовые
X_train,X_test,y_train,y_test = train_test_split(df['text'], df.label, test_size=0.2, random_state=7)

# Получим векторные представления текстов
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)

tfidf_train = tfidf_vectorizer.fit_transform(X_train)
tfidf_test = tfidf_vectorizer.transform(X_test)

#Создаем модель с помощью PAC
pac = PassiveAggressiveClassifier(max_iter=50)
pac.fit(tfidf_train,y_train)

#Проверяем модель на тестовых данных
y_pred = pac.predict(tfidf_test)

#Выводим точность предсказания модели
score = accuracy_score(y_test,y_pred)
print('Точность: %.2f%%' % (score * 100))

#Построим матрицу ошибок
fig, ax = plt.subplots(figsize=(10, 5))
ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)

print('\n Clasification report:\n', classification_report(y_test,y_pred))

#Проверим модель на произвольных данных не из датасета
ii=['This is a really important question, Lambert says. “I don’t want to be passed along to two or three people,” she says. “I want one person to contact.” There may be specific contact points for different areas, she adds, such as the director of nursing for related questions. However, “I want to know that I can pop into the executive director’s office anytime, ask any question and make any kind of complaint,” she emphasizes. “I want to know that person is available. Because sometimes, you have to go up to that level.""']

ii= tfidf_vectorizer.transform(ii)

y_pred=pac.predict(ii)

y_pred